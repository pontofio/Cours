{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pontofio/Cours/blob/main/Copie_de_Notebook_1_Regression_Lineaire_Etudiant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a25cad7b",
      "metadata": {
        "id": "a25cad7b"
      },
      "source": [
        "# üè† R√©gression Lin√©aire ‚Äì California Housing\n",
        "üë®‚Äçüè´ Professeur : Dr. Khalil HADDAOUI\n",
        "\n",
        "**Probl√®me concret** : Estimer le **prix m√©dian** d'un quartier √† partir de caract√©ristiques locales (revenu m√©dian, densit√©, proximit√© oc√©an, etc.).  \n",
        "**Objectifs** : pipeline propre, baseline, am√©lioration (polyn√¥mes, Ridge/Lasso), √©valuation (RMSE/MAE/R¬≤).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7edcf817",
      "metadata": {
        "id": "7edcf817"
      },
      "source": [
        "## üìê Mod√®le lin√©aire multiple\n",
        "$$\n",
        "\\hat y = \\beta_0 + \\sum_{j=1}^p \\beta_j x_j\n",
        "$$\n",
        "Estimation par moindres carr√©s (minimise la somme des carr√©s des r√©sidus).  \n",
        "**R√©gularisation** : Ridge (L2) et Lasso (L1) r√©duisent la variance et l'overfit.\n",
        "\n",
        "- Ridge : $\\min_\\beta \\|y-X\\beta\\|_2^2 + \\alpha \\|\\beta\\|_2$\n",
        "- Lasso : $\\min_\\beta \\|y-X\\beta\\|_2^2 + \\alpha \\|\\beta\\|_1$\n",
        "\n",
        "**Pourquoi ?** Donn√©es bruit√©es / corr√©l√©es ‚Üí coefficients instables ‚Üí la r√©gularisation stabilise et g√©n√©ralise mieux.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f9825fe",
      "metadata": {
        "id": "2f9825fe",
        "outputId": "cd7b2eb3-9f34-49b3-8fdf-165baabf8360",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
              " 0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
              " 1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
              " 2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
              " 3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
              " 4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
              " \n",
              "    Longitude  \n",
              " 0    -122.23  \n",
              " 1    -122.22  \n",
              " 2    -122.24  \n",
              " 3    -122.25  \n",
              " 4    -122.25  ,\n",
              " 0    4.526\n",
              " 1    3.585\n",
              " 2    3.521\n",
              " 3    3.413\n",
              " 4    3.422\n",
              " Name: MedHouseVal, dtype: float64)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X.head(), y.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f154487",
      "metadata": {
        "id": "6f154487",
        "outputId": "049b9c91-cbdd-4f3f-8e0c-d594af3b941b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline -> RMSE: 1.3106960720039365 | MAE: 0.9060685490007149 | R2: -0.00021908714592466794\n"
          ]
        }
      ],
      "source": [
        "# Baseline: pr√©dire la moyenne du train\n",
        "import numpy as np\n",
        "yhat_mean = np.full_like(y_test, y_train.mean())\n",
        "rmse = mean_squared_error(y_test, yhat_mean)\n",
        "mae = mean_absolute_error(y_test, yhat_mean)\n",
        "r2  = r2_score(y_test, yhat_mean)\n",
        "print(\"Baseline -> RMSE:\", rmse, \"| MAE:\", mae, \"| R2:\", r2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eefb4c64",
      "metadata": {
        "id": "eefb4c64",
        "outputId": "2b002dab-6dde-4837-bcbc-3950d86e174a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV RMSE (LinearRegression): 0.7205271873526421\n",
            "Test RMSE: 0.5558915986952442\n",
            "Test MAE : 0.5332001304956565\n",
            "Test R2  : 0.575787706032451\n"
          ]
        }
      ],
      "source": [
        "# TODO: construire un pipeline standardisation + r√©gression lin√©aire\n",
        "pipe_lr = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", LinearRegression())\n",
        "])\n",
        "\n",
        "cv_rmse = (-cross_val_score(pipe_lr, X_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=5)).mean()\n",
        "print(\"CV RMSE (LinearRegression):\", cv_rmse)\n",
        "\n",
        "# TODO: entra√Æner et √©valuer sur test (RMSE, MAE, R2)\n",
        "pipe_lr.fit(X_train, y_train)\n",
        "yhat = pipe_lr.predict(X_test)\n",
        "print(\"Test RMSE:\", mean_squared_error(y_test,yhat))\n",
        "print(\"Test MAE :\", mean_absolute_error(y_test,yhat))\n",
        "print(\"Test R2  :\", r2_score(y_test,yhat))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cbd71e8",
      "metadata": {
        "id": "4cbd71e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e00ec9-a1c0-4218-aea9-b1c7ce2d6c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV RMSE (Poly+Ridge): 3.125718216312988\n",
            "Test RMSE: 0.68021523026453\n",
            "Test MAE : 0.4670525215371786\n",
            "Test R2  : 0.6469096540341559\n"
          ]
        }
      ],
      "source": [
        "# TODO: ajouter des features polynomiales (degr√©=2) et comparer\n",
        "pipe_poly = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    (\"model\", Ridge(alpha=1.0))\n",
        "])\n",
        "cv_rmse_poly = (-cross_val_score(pipe_poly, X_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=3)).mean()\n",
        "print(\"CV RMSE (Poly+Ridge):\", cv_rmse_poly)\n",
        "\n",
        "pipe_poly.fit(X_train, y_train)\n",
        "yhat_poly = pipe_poly.predict(X_test)\n",
        "print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, yhat_poly)))\n",
        "print(\"Test MAE :\", mean_absolute_error(y_test, yhat_poly))\n",
        "print(\"Test R2  :\", r2_score(y_test, yhat_poly))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0da70be0",
      "metadata": {
        "id": "0da70be0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "d79b3883-b1a0-43b3-a504-94634531999c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Pipeline' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1050289337.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: GridSearch sur Ridge (alpha) # le alpha du ridge ou lasso est ce qu'on appelle un hyperparm√®tre ==> on applique un GridSearchCV pour trouver la meilleur valeur de ce param√®t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"model__alpha\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpipe_ridge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scaler\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRidge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe_ridge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"neg_root_mean_squared_error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# TODO: gs.fit(...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"
          ]
        }
      ],
      "source": [
        "# TODO: GridSearch sur Ridge (alpha) # le alpha du ridge ou lasso est ce qu'on appelle un hyperparm√®tre ==> on applique un GridSearchCV pour trouver la meilleur valeur de ce param√®t\n",
        "param_grid = {\"model__alpha\": [0.1, 1.0, 10.0]}\n",
        "pipe_ridge = Pipeline([(\"scaler\", StandardScaler()), (\"model\", Ridge())])\n",
        "gs = GridSearchCV(pipe_ridge, param_grid, scoring=\"neg_root_mean_squared_error\", cv=5, n_jobs=-1)\n",
        "# TODO: gs.fit(...)\n",
        "gs.fit(X_train, y_train)\n",
        "\n",
        "print(\"Meilleur alpha:\", gs.best_params_[\"model__alpha\"])\n",
        "#best = gs.best_estimator_\n",
        "#print(\"Best = \", best)\n",
        "yhat_best = best.predict(X_test)\n",
        "\n",
        "print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, yhat_best)))\n",
        "print(\"Test MAE :\", mean_absolute_error(y_test, yhat_best))\n",
        "print(\"Test R2  :\", r2_score(y_test, yhat_best))\n",
        "\n",
        "#print(\"Best Test RMSE:\", mean_squared_error(y_test, yhat_best, squared=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9045dba",
      "metadata": {
        "id": "d9045dba"
      },
      "source": [
        "### ‚úÖ √Ä retenir\n",
        "- Toujours une baseline.\n",
        "- Pipelines pour √©viter les fuites de donn√©es.\n",
        "- R√©gulariser si corr√©lations/overfit.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}